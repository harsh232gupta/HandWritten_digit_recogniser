# -*- coding: utf-8 -*-
"""HandWritten_digit_recogniser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q_-bSTBcQ9EIZvzLna69kR7Vqim4ERx7
"""

import tensorflow as tf
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Activation
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.optimizers import Adam
import numpy as np

mnist = tf.keras.datasets.mnist

(x_train,y_train),(x_test,y_test) = mnist.load_data()

x_train.shape

x_test.shape

x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, test_size=0.99, random_state=1)

x_train.shape

x_cv.shape

x_test.shape

x_train = tf.keras.utils.normalize(x_train)
x_test = tf.keras.utils.normalize(x_test)
x_cv = tf.keras.utils.normalize(x_cv)

print(x_train[0])

print(x_cv[0])

plt.imshow(x_train[0],cmap = plt.cm.binary)

plt.imshow(x_cv[0],cmap = plt.cm.binary)

print(y_cv[0])

print(y_train[0])

x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

x_cv = x_cv.reshape(x_cv.shape[0],-1)

x_train.shape

y_cv.shape

x_cv.shape

tf.random.set_seed(1234)

# Define lambdas and initialize models list
lambdas = [0.0, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3]
models = [None] * len(lambdas)

# Train models with different lambda values
for i in range(len(lambdas)):
    lambda_ = lambdas[i]
    models[i] = Sequential([
        Dense(120, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(lambda_)),
        Dense(40, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(lambda_)),
        Dense(10, activation='linear')
    ])
    models[i].compile(
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        optimizer=tf.keras.optimizers.Adam(0.01),
    )
    models[i].fit(
        x_train, y_train,
        epochs=10,
        verbose=0  # Add verbose=0 to suppress output for brevity
    )
    print(f"Finished lambda = {lambda_}")

# Define eval_cat_error function
def eval_cat_error(y, yhat):
    error = np.mean(y != yhat)
    return error

# Define plot_iterate function
def plot_iterate(lambdas, models, x_train, y_train, x_cv, y_cv):
    train_errors = []
    cv_errors = []

    for model in models:
        # Predict on training data
        prediction_train = model.predict(x_train)
        yhat_train = np.argmax(prediction_train, axis=1)

        # Compute training error
        train_error = eval_cat_error(y_train, yhat_train)
        train_errors.append(train_error)

        # Predict on cross-validation data
        prediction_cv = model.predict(x_test)
        yhat_cv = np.argmax(prediction_cv, axis=1)

        # Compute cross-validation error
        cv_err = eval_cat_error(y_test, yhat_cv)
        cv_errors.append(cv_err)

    # Plot the errors
    plt.plot(lambdas, train_errors, label='Train')
    plt.plot(lambdas, cv_errors, label='Cross-validation')
    plt.xlabel('Lambda')
    plt.ylabel('Error')
    plt.title('Training and Cross-validation Errors vs Lambda')
    plt.legend()
    plt.show()

# Assuming x_train, y_train, x_cv, y_cv are defined and properly preprocessed
plot_iterate(lambdas, models, x_train, y_train, x_cv, y_cv)

